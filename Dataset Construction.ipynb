{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b17ae3",
   "metadata": {},
   "source": [
    "# <p style='text-align: center'>Data from multiple datasets need to be normalized and aggregated into a single train/test set. Metadata will be collected about the train/test data in a new .csv spreadsheet</p><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8449ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl as op\n",
    "import numpy as np\n",
    "import math\n",
    "import shutil\n",
    "import cv2\n",
    "# Silence TensorFlow from complaining about my NUMA nodes not being readable\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import imblearn\n",
    "import math\n",
    "import filecmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7257800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualmed path\n",
    "actmed_image_path = './dataset/Actualmed-COVID-chestxray-dataset/images/'\n",
    "actmed_meta_path = './dataset/Actualmed-COVID-chestxray-dataset/metadata.csv'\n",
    "# Sirm path\n",
    "sirm_image_covid_path = './dataset/COVID-19-Radiography-Database/COVID/'\n",
    "sirm_image_normal_path = './dataset/COVID-19-Radiography-Database/NORMAL/'\n",
    "sirm_image_viral_path = './dataset/COVID-19-Radiography-Database/Viral-Pneumonia/'\n",
    "sirm_meta_covid_path = './dataset/COVID-19-Radiography-Database/COVID.metadata.xlsx'\n",
    "sirm_meta_normal_path = './dataset/COVID-19-Radiography-Database/NORMAL.metadata.xlsx'\n",
    "sirm_meta_viral_path = './dataset/COVID-19-Radiography-Database/Viral-Pneumonia.metadata.xlsx'\n",
    "# covid-chestxray-dataset path\n",
    "cohen_image_path = './dataset/covid-chestxray-dataset/images/'\n",
    "cohen_meta_path = './dataset/covid-chestxray-dataset/metadata.csv'\n",
    "# Figure1 path\n",
    "fig1_image_path = './dataset/Figure1-COVID-chestxray-dataset/images/'\n",
    "fig1_meta_path = './dataset/Figure1-COVID-chestxray-dataset/metadata.csv'\n",
    "# ricord path\n",
    "ricord_image_path = './dataset/ricord/images/'\n",
    "ricord_meta_path = './dataset/ricord/ricord_meta.csv'\n",
    "\n",
    "# rsna path\n",
    "rsna_image_path = './dataset/rsna/images/'\n",
    "rsna_meta_path = './dataset/rsna/rsna_meta.csv'\n",
    "\n",
    "# aggregated dataset output paths\n",
    "dataset_out_train_path = './dataset/train/'\n",
    "dataset_out_validate_path = './dataset/validate/'\n",
    "dataset_out_test_path = './dataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b06730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recordkeeping for final dataset\n",
    "train = []\n",
    "validate = []\n",
    "test = []\n",
    "data_count = {'NORMAL': 0, 'PNEUMONIA': 0, 'COVID-19': 0}\n",
    "dataset = {'actmed': [], 'sirm': [], 'cohen': [], 'fig1': [], 'ricord': [], 'rsna': []}\n",
    "patient_list = []\n",
    "\n",
    "results = dict()\n",
    "results['COVID-19'] = 'COVID-19'\n",
    "results['Pneumonia'] = 'PNEUMONIA'\n",
    "results['SARS'] = 'PNEUMONIA'\n",
    "results['Pneumocystis'] = 'PNEUMONIA'\n",
    "results['Streptococcus'] = 'PNEUMONIA'\n",
    "results['Chlamydophila'] = 'PNEUMONIA'\n",
    "results['E.Coli'] = 'PNEUMONIA'\n",
    "results['Klebsiella'] = 'PNEUMONIA'\n",
    "results['Legionella'] = 'PNEUMONIA'\n",
    "results['Lipoid'] = 'PNEUMONIA'\n",
    "results['Varicella'] = 'PNEUMONIA'\n",
    "results['Bacterial'] = 'PNEUMONIA'\n",
    "results['Mycoplasma'] = 'PNEUMONIA'\n",
    "results['Influenza'] = 'PNEUMONIA'\n",
    "results['Tuberculosis'] = 'PNEUMONIA'\n",
    "results['H1N1'] = 'PNEUMONIA'\n",
    "results['Aspergillosis'] = 'PNEUMONIA'\n",
    "results['Herpes'] = 'PNEUMONIA'\n",
    "results['Aspiration'] = 'PNEUMONIA'\n",
    "results['Nocardia'] = 'PNEUMONIA'\n",
    "results['MERS-CoV'] = 'PNEUMONIA'\n",
    "results['MRSA'] = 'PNEUMONIA'\n",
    "results['No Finding'] = 'NORMAL'\n",
    "results['No finding'] = 'NORMAL'\n",
    "results['Normal'] = 'NORMAL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d4c62",
   "metadata": {},
   "source": [
    "Next, go through each dataset explicitly, log patientid, finding, and filename per each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019fd0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActualMed Added 51 entries.\n",
      "{'NORMAL': 0, 'PNEUMONIA': 0, 'COVID-19': 51}\n"
     ]
    }
   ],
   "source": [
    "# ActualMed\n",
    "# Note: can only take Covid positive cases as this set characterizes any other non-normal case as no finding\n",
    "actmed_meta = pd.read_csv(actmed_meta_path)\n",
    "new_entries = 0\n",
    "for index, series in actmed_meta.iterrows():\n",
    "    if not series['finding'] == 'NaN' and series['finding'] == 'COVID-19' and series['patientid'] not in patient_list:\n",
    "        dataset['actmed'].append([series['patientid'], series['finding'], os.path.join(actmed_image_path, series['imagename'])])\n",
    "        patient_list.append(series['patientid'])\n",
    "        data_count['COVID-19'] += 1\n",
    "        new_entries += 1\n",
    "print('ActualMed Added', new_entries, 'entries.')\n",
    "print(data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb56034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIRM Added 3885 entries.\n",
      "{'NORMAL': 1341, 'PNEUMONIA': 1345, 'COVID-19': 1250}\n"
     ]
    }
   ],
   "source": [
    "# sirm\n",
    "# normal\n",
    "new_entries = 0\n",
    "sirm_meta_normal = pd.read_excel(sirm_meta_normal_path)\n",
    "for index, series in sirm_meta_normal.iterrows():\n",
    "    if series['FILE NAME'] in patient_list:\n",
    "        continue\n",
    "    dataset['sirm'].append([series['FILE NAME'], 'NORMAL' , os.path.join(sirm_image_normal_path, series['FILE NAME'].split('-')[0] + ' (' + str(index + 1) + ').' + series['FORMAT'].lower())])\n",
    "    data_count['NORMAL'] += 1\n",
    "    new_entries += 1\n",
    "sirm_meta_viral = pd.read_excel(sirm_meta_viral_path)\n",
    "# pneumonia\n",
    "for index, series in sirm_meta_viral.iterrows():\n",
    "    if series['FILE NAME'] in patient_list:\n",
    "        continue\n",
    "    dataset['sirm'].append([series['FILE NAME'], 'PNEUMONIA' , os.path.join(sirm_image_viral_path, series['FILE NAME'].split('-')[0] + ' (' + str(index + 1) + ').' + series['FORMAT'].lower())])\n",
    "    data_count['PNEUMONIA'] += 1\n",
    "    new_entries += 1\n",
    "sirm_meta_covid = pd.read_excel(sirm_meta_covid_path)\n",
    "# covid\n",
    "for index, series in sirm_meta_covid.iterrows():\n",
    "    if series['FILE NAME'] in patient_list:\n",
    "        continue\n",
    "    dataset['sirm'].append([str(series['FILE NAME']), 'COVID-19' , os.path.join(sirm_image_covid_path, series['FILE NAME'].split()[0] + ' (' + str(index + 1) + ').' + series['FORMAT'].lower())])\n",
    "    data_count['COVID-19'] += 1\n",
    "    new_entries += 1\n",
    "print('SIRM Added', new_entries, 'entries.')\n",
    "print(data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b2f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Added 339 entries.\n",
      "{'NORMAL': 1355, 'PNEUMONIA': 1453, 'COVID-19': 1467}\n"
     ]
    }
   ],
   "source": [
    "# cohen\n",
    "# Note: only accepting AP and PA views\n",
    "new_entries = 0\n",
    "# We only want one instance of each patient to maintain fair weighting in the final dataset\n",
    "included_patients = []\n",
    "cohen_meta = pd.read_csv(cohen_meta_path)\n",
    "for index, series in cohen_meta.iterrows():\n",
    "    finding = series['finding'].split('/')[-1]\n",
    "    if finding in results:\n",
    "        if series['view'] == 'AP' or series['view'] == 'PA':\n",
    "            # Slice last character off of patientid if it has a non digit character at the end\n",
    "            if series['patientid'].isdigit():\n",
    "                patient_id = series['patientid']\n",
    "            else:\n",
    "                patient_id = str(series['patientid'])[:-1]\n",
    "            if not patient_id in included_patients and 'Cohen' + patient_id not in patient_list:   \n",
    "                dataset['cohen'].append(['Cohen' + patient_id, results[finding], os.path.join(cohen_image_path, series['filename'])])\n",
    "                data_count[results[finding]] += 1\n",
    "                new_entries += 1\n",
    "                included_patients.append(patient_id)\n",
    "                \n",
    "print('Cohen Added', new_entries, 'entries.')\n",
    "print(data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130fbb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure1 Added 40 entries.\n",
      "{'NORMAL': 1358, 'PNEUMONIA': 1455, 'COVID-19': 1502}\n"
     ]
    }
   ],
   "source": [
    "# Figure1\n",
    "new_entries = 0\n",
    "# Will not bother filtering out images from the same patient as they were taken on different days and are not identical\n",
    "fig1_meta = pd.read_csv(fig1_meta_path, encoding='ISO-8859-1')\n",
    "for index, series in fig1_meta.iterrows():\n",
    "    if series['finding'] in results and series['patientid'] not in included_patients and series['patientid'] not in patient_list:\n",
    "        # Imagename not provided so we need to check if either the .jpg or .png exists first\n",
    "        if os.path.exists(os.path.join(fig1_image_path, series['patientid'] + '.jpg')):\n",
    "            dataset['fig1'].append([series['patientid'], results[series['finding']], os.path.join(fig1_image_path, series['patientid'] + '.jpg')])\n",
    "        elif os.path.exists(os.path.join(fig1_image_path, series['patientid'] + '.png')):\n",
    "            dataset['fig1'].append([series['patientid'], results[series['finding']], os.path.join(fig1_image_path, series['patientid'] + '.png')])\n",
    "        data_count[results[series['finding']]] += 1\n",
    "        new_entries += 1\n",
    "print('Figure1 Added', new_entries, 'entries.')\n",
    "print(data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b8584",
   "metadata": {},
   "source": [
    "# <hr><p style='text-align: center'>Run 'Format Dicom Datasets.ipynb' before continuing!</p><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b965dab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RICORD Added 1130 entries.\n",
      "{'NORMAL': 1358, 'PNEUMONIA': 1455, 'COVID-19': 2632}\n"
     ]
    }
   ],
   "source": [
    "# RICORD\n",
    "new_entries = 0\n",
    "# Meta file is created exactly how we need it so no field normalization required\n",
    "ricord_meta = pd.read_csv(ricord_meta_path)\n",
    "for index, series in ricord_meta.iterrows():\n",
    "    if str(series['finding']) in results and series['patientid'] not in included_patients and series['patientid'] not in patient_list:\n",
    "        dataset['ricord'].append([series['patientid'], results[series['finding']], os.path.join(ricord_image_path, series['imagename'])])\n",
    "        data_count[results[series['finding']]] += 1\n",
    "        new_entries += 1\n",
    "print('RICORD Added', new_entries, 'entries.')\n",
    "print(data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fcad97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSNA Added 14863 entries.\n",
      "{'NORMAL': 10209, 'PNEUMONIA': 7467, 'COVID-19': 2632}\n"
     ]
    }
   ],
   "source": [
    "# RSNA\n",
    "new_entries = 0\n",
    "# Meta file is created exactly how we need it so no field normalization required\n",
    "ricord_meta = pd.read_csv(rsna_meta_path)\n",
    "for index, series in ricord_meta.iterrows():\n",
    "    if str(series['finding']) in results and series['patientid'] not in included_patients and series['patientid'] not in patient_list:\n",
    "        dataset['ricord'].append([series['patientid'], results[series['finding']], os.path.join(rsna_image_path, series['imagename'])])\n",
    "        data_count[results[series['finding']]] += 1\n",
    "        new_entries += 1\n",
    "print('RSNA Added', new_entries, 'entries.')\n",
    "print(data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d8cd3",
   "metadata": {},
   "source": [
    "# <p style='text-align: center'>Now create train-validate-test sets</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "790e3d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train: 14214\n",
      "validate: 4060\n",
      "    test: 2034\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "validate_ratio = 0.2\n",
    "#test_ratio = 0.1\n",
    "\n",
    "# For reliable reproduction of results\n",
    "#np.random.seed(0)\n",
    "\n",
    "# Add all datasets to an aggregate set and shuffle order\n",
    "aggregate_list = []\n",
    "\n",
    "for items in dataset.values():\n",
    "    for item in items:\n",
    "        aggregate_list.append(item)\n",
    "\n",
    "np.random.shuffle(aggregate_list)\n",
    "\n",
    "normal_list = []\n",
    "pneumonia_list = []\n",
    "covid_list = []\n",
    "\n",
    "for entry in aggregate_list:\n",
    "    if entry[1] == 'NORMAL':\n",
    "        normal_list.append(entry)\n",
    "    elif entry[1] == 'PNEUMONIA':\n",
    "        pneumonia_list.append(entry)\n",
    "    else:\n",
    "        covid_list.append(entry)\n",
    "\n",
    "# Slice 70% of each list into the train set\n",
    "train.extend(normal_list[:math.floor(train_ratio * len(normal_list))])\n",
    "train.extend(pneumonia_list[:math.floor(train_ratio * len(pneumonia_list))])\n",
    "train.extend(covid_list[:math.floor(train_ratio * len(covid_list))])\n",
    "\n",
    "# Slice next 20% of each list into the validate set\n",
    "validate.extend(normal_list[\n",
    "    math.floor(train_ratio * len(normal_list)):\n",
    "    math.floor(train_ratio * len(normal_list)) + math.floor(validate_ratio * len(normal_list))])\n",
    "validate.extend(pneumonia_list[\n",
    "    math.floor(train_ratio * len(pneumonia_list)):\n",
    "    math.floor(train_ratio * len(pneumonia_list)) + math.floor(validate_ratio * len(pneumonia_list))])\n",
    "validate.extend(covid_list[\n",
    "    math.floor(train_ratio * len(covid_list)):\n",
    "    math.floor(train_ratio * len(covid_list)) + math.floor(validate_ratio * len(covid_list))])\n",
    "\n",
    "# Slice the remaining 10% into the test set\n",
    "test.extend(normal_list[\n",
    "    math.floor(train_ratio * len(normal_list)) + math.floor(validate_ratio * len(normal_list)):])\n",
    "test.extend(pneumonia_list[\n",
    "    math.floor(train_ratio * len(pneumonia_list)) + math.floor(validate_ratio * len(pneumonia_list)):])\n",
    "test.extend(covid_list[\n",
    "    math.floor(train_ratio * len(covid_list)) + math.floor(validate_ratio * len(covid_list)):])\n",
    "\n",
    "\n",
    "print('   train: {}'.format(len(train)))\n",
    "print('validate: {}'.format(len(validate)))\n",
    "print('    test: {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d42f7",
   "metadata": {},
   "source": [
    "# <p style='text-align: center'>Finally copy the aggregate datasets into their respective directories</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb0a583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure our output directories exist\n",
    "os.makedirs(os.path.join(dataset_out_train_path,'images','NORMAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_out_train_path,'images','PNEUMONIA'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_out_train_path,'images','COVID-19'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_out_validate_path,'images','NORMAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_out_validate_path,'images','PNEUMONIA'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_out_validate_path,'images','COVID-19'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_out_test_path,'images','NORMAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_out_test_path,'images','PNEUMONIA'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_out_test_path,'images','COVID-19'), exist_ok=True)\n",
    "\n",
    "train_df = pd.DataFrame(columns=['patientid','finding','imagename'])\n",
    "validate_df = pd.DataFrame(columns=['patientid','finding','imagename'])\n",
    "test_df = pd.DataFrame(columns=['patientid','finding','imagename'])\n",
    "\n",
    "# CLAHE Instance Definition\n",
    "clahe = cv2.createCLAHE(clipLimit = 1.25)\n",
    "\n",
    "for datapoint in train:\n",
    "    # apply CLAHE to each image\n",
    "    image = cv2.cvtColor(cv2.imread(datapoint[2]).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    clahe_image = clahe.apply(image)\n",
    "    # Center crop to 90% original size to cut off arms/non-chest areas\n",
    "    center = (clahe_image.shape[0] / 2, clahe_image.shape[1] / 2)\n",
    "    w_crop = clahe_image.shape[0] * 0.9\n",
    "    h_crop = clahe_image.shape[1] * 0.9\n",
    "    clahe_image = clahe_image[math.floor(center[0] - w_crop / 2):math.floor(center[0] + w_crop / 2), math.floor(center[1] - h_crop / 2):math.floor(center[1] + h_crop / 2)]\n",
    "    # Resize to (300, 300)\n",
    "    clahe_image = cv2.resize(clahe_image, (300, 300), interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imwrite(os.path.join(dataset_out_train_path,'images', datapoint[1], datapoint[0] + '.png'), clahe_image)\n",
    "    # print(clahe_image.shape)\n",
    "    # shutil.copy(os.path.join('./dataset/cache/', datapoint[0] + '.png'), os.path.join(dataset_out_train_path,'images', datapoint[1]))\n",
    "    train_df = train_df.append({'patientid': datapoint[0], 'finding': datapoint[1], 'imagename': os.path.join(dataset_out_train_path, 'images', datapoint[1], datapoint[0] + '.png')}, ignore_index=True)\n",
    "for datapoint in validate:\n",
    "    # apply CLAHE to each image\n",
    "    image = cv2.cvtColor(cv2.imread(datapoint[2]).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    clahe_image = clahe.apply(image)\n",
    "    # Center crop to 90% original size to cut off arms/non-chest areas\n",
    "    center = (clahe_image.shape[0] / 2, clahe_image.shape[1] / 2)\n",
    "    w_crop = clahe_image.shape[0] * 0.9\n",
    "    h_crop = clahe_image.shape[1] * 0.9\n",
    "    clahe_image = clahe_image[math.floor(center[0] - w_crop / 2):math.floor(center[0] + w_crop / 2), math.floor(center[1] - h_crop / 2):math.floor(center[1] + h_crop / 2)]\n",
    "    # Resize to (300, 300)\n",
    "    clahe_image = cv2.resize(clahe_image, (300, 300), interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imwrite(os.path.join(dataset_out_validate_path,'images', datapoint[1], datapoint[0] + '.png'), clahe_image)\n",
    "    # print(clahe_image.shape)\n",
    "    # shutil.copy(os.path.join('./dataset/cache/', datapoint[0] + '.png'), os.path.join(dataset_out_validate_path,'images', datapoint[1]))\n",
    "    validate_df = validate_df.append({'patientid': datapoint[0], 'finding': datapoint[1], 'imagename': os.path.join(dataset_out_train_path, 'images', datapoint[1], datapoint[0] + '.png')}, ignore_index=True)\n",
    "for datapoint in test:\n",
    "    # apply CLAHE to each image\n",
    "    image = cv2.cvtColor(cv2.imread(datapoint[2]).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    clahe_image = clahe.apply(image)\n",
    "    # Center crop to 90% original size to cut off arms/non-chest areas\n",
    "    center = (clahe_image.shape[0] / 2, clahe_image.shape[1] / 2)\n",
    "    w_crop = clahe_image.shape[0] * 0.9\n",
    "    h_crop = clahe_image.shape[1] * 0.9\n",
    "    clahe_image = clahe_image[math.floor(center[0] - w_crop / 2):math.floor(center[0] + w_crop / 2), math.floor(center[1] - h_crop / 2):math.floor(center[1] + h_crop / 2)]\n",
    "    # Resize to (300, 300)\n",
    "    clahe_image = cv2.resize(clahe_image, (300, 300), interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imwrite(os.path.join(dataset_out_test_path,'images', datapoint[1], datapoint[0] + '.png'), clahe_image)\n",
    "    # print(clahe_image.shape)\n",
    "    # shutil.copy(os.path.join('./dataset/cache/', datapoint[0] + '.png'), os.path.join(dataset_out_test_path,'images', datapoint[1]))\n",
    "    test_df = test_df.append({'patientid': datapoint[0], 'finding': datapoint[1], 'imagename': os.path.join(dataset_out_train_path, 'images', datapoint[1], datapoint[0] + '.png')}, ignore_index=True)\n",
    "\n",
    "train_df.to_csv(os.path.join(dataset_out_train_path,'metadata.csv'), index=False)\n",
    "validate_df.to_csv(os.path.join(dataset_out_validate_path,'metadata.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(dataset_out_test_path,'metadata.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
